---
title: "Manipulation de données massives"
editor: visual
---

# Handling Massive Data with R

The objective of this session is to familiarize you with the essential principles and techniques for working with massive data. The tools we will explore each have their advantages and disadvantages, and the choice to use them will depend primarily on the specific needs of your projects. It is important to note that some of these tools require robust computer infrastructures that may not be available for all projects.

To illustrate these techniques, we will use data from the 2017 French population census. Although these data may not be considered massive in themselves, they are already voluminous enough to pose memory management challenges on some computers. Our goal is to provide you with the necessary skills to manipulate large quantities of data, which you can then apply to your own use cases, whether they are large or small in scale.

During this practical session, we will explore two main approaches. First, we will examine how to efficiently manipulate massive datasets using **local** resources, even when the RAM capacity is less than the size of the analyzed data. Next, we will delve into data manipulation directly from the **cloud**, when local resources are no longer sufficient to meet your needs.

## I - Local Data Manipulation

Before we begin, let's load some libraries that will be useful throughout the practical session.

```{r}
library(ggplot2)
library(dplyr)
```

### A - Importing and Storing Massive Data

#### Importation

When dealing with *Big Data*, the choice of storage format is crucial to optimize the efficiency and performance of operations. The most commonly used format for data storage is `.csv`. This format has the advantage of being easily readable, both by humans and various computer languages and systems. However, it has some major drawbacks: it is not compressed, which means it occupies a lot of disk space, and it must be loaded entirely into memory even when we want to analyze only a subset of the data. This requirement has implications for both storage and computation time.

In this practical session, we will explore an increasingly popular alternative to the `.csv` format: the `.parquet` format. This format has been chosen as the default storage and distribution format for the coming years by INSEE.

To start, we will import the `tp/data/data_recensement_2017.csv` file provided, which contains the 2017 census data. We will use three different methods for importing, and we will analyze their respective performances:

-   importation via the standard `readr` package.
-   importation via the `data.table` package, which is an extension to the base R data.frame.
-   importation via the `arrow` package.

The goal is to understand how each method efficiently handles (massive) data, considering memory occupancy and the time required for importation. This step is crucial for working with massive data efficiently. Start by importing the data using these three different approaches.


```{r}
# Importation with readr
# quite long (~4-5min)
t <- Sys.time()
df_readr <- readr::read_delim(
  "data/data_recensement_2017.csv",
  delim = ";",
  col_types = readr::cols(COMMUNE = readr::col_character())
)
elapsed_readr <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```


The import via `readr` is particularly time-consuming. Moreover, it is interesting to note the memory usage: the data is "only" 4.4 GB, but the memory usage reaches 30 GB during reading! This highlights the general need to allocate ample RAM compared to the size of the data. It emphasizes the importance of exploring more efficient methods for loading data, as we will in the following parts of this practical session.

It is wise to remove heavy objects that are no longer needed in the R environment. When you create objects in R, they occupy space in memory. Accumulating numerous objects without removing them can quickly saturate the available memory, leading to slowdowns or crashes in your R session. By deleting unnecessary objects using the `rm()` function, you free up memory space for other tasks or analyses. Moreover, the less cluttered the memory, the faster the performance.

```{r}
# Free up memory space
rm(df_readr)
```

```{r}
# Importation with data.table
t <- Sys.time()
df_datatable <- data.table::fread(
  "data/data_recensement_2017.csv",
  sep = ";",
  colClasses = c(
    COMMUNE = "character"
  ),
  data.table = FALSE
)
elapsed_datatable <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

The import is much faster. Additionally, the maximum RAM usage reached is about 20 GB. This highlights the benefit of using tools specifically designed for large datasets. However, `data.table` has a significant limitation: it uses a syntax very different from that of `dplyr`. In this practical session, we will primarily use the `arrow` package, which has the dual advantage of being very memory-efficient and widely compatible with the `dplyr` syntax.

```{r}
# Importer with arrow
t <- Sys.time()
df_arrow <- arrow::read_delim_arrow(
  "data/data_recensement_2017.csv",
  delim = ";",
  col_types = arrow::schema(COMMUNE = arrow::utf8()),
  as_data_frame = TRUE
)
elapsed_arrow <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

With `arrow`, the import is not only extremely fast, but the memory usage is also minimal (approximately 13 GB). It's not surprising that this package is becoming a reference for processing large datasets!

```{r}
head(df_arrow, 6)
```

We free up memory
```{r}
rm(df_datatable)
```

Let's analyze the differences in CSV import times between the 3 methods.

```{r}
performance <- tibble(
  format_text = c("readr", "data.table", "arrow"),
  elapse_time = round(c(elapsed_readr, elapsed_datatable, elapsed_arrow), 3)
)

performance |>
  mutate(format_text = forcats::fct_reorder(format_text, elapse_time)) |>
  ggplot(aes(format_text, elapse_time)) +
  geom_col(fill = "#003299", width = 0.6) +
  geom_text(aes(label = glue::glue("{elapse_time} sec")), hjust = -0.1, size = 4, fontface = "bold") +
  expand_limits(y = c(0, 200)) +
  coord_flip() +
  labs(
    title = "Performance of CSV File Import",
    subtitle = "between readr, data.table, and arrow",
    x = "",
    y = "Import Time in Seconds"
  ) +
  theme_light()
```

As mentioned earlier, the `csv` format is not the ideal option for handling massive data. Therefore, it is strongly recommended to work with data in `parquet` format or store them in a database, depending on the specific needs of your project. It's worth noting that, depending on use cases, working with partitioned `parquet` files rather than a single large file can be a wise choice. This approach allows for more efficient management of massive data while improving processing performance.

#### Storage

Now that we have imported our CSV file into memory, let's move on to the next step: writing the file `data/data_recensement_2017.parquet`.

```{r}
# Writing of an unique parquet file
arrow::write_parquet(df_arrow, "data/data_recensement_2017.parquet")
```

On peut également choisir de partitionner notre fichier parquet, par exemple par région. On exporte le fichier partionné dans un dossier `data/data_partition`.

```{r}
# Writing a directory containing partitioned parquet files by region
arrow::write_dataset(df_arrow,
  "data/data_partition",
  format = "parquet",
  partitioning = c("REGION")
)
```

Take the time to examine how the parquet files are saved in the `data_partition` folder.

```{r}
fs::dir_tree("data/data_partition")
```

Another effective method for storing our data is to integrate it into a DuckDB database, allowing us to perform SQL queries later on. Although DuckDB cannot yet open multi-level files created by partition, it has the capability to directly read CSV or Parquet files. However, it's important to note that, like any database, the `.duckdb` format can contain multiple tables and execute all standard database management operations.

To create a database from R, you can use the `duckdb()` function, specifying the path where you want to save it.

```{r}
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "data/data_recensement_2017.duckdb", read_only = FALSE)
```

One of the main advantages of DuckDB is its interoperability with Arrow, making it very easy to switch between an Arrow object and a DuckDB object, and vice versa. To achieve this, you can use the `to_duckdb()` and `to_arrow()` functions.

```{r}
arrow::to_duckdb(df_arrow, table_name = "RP2017", con = con)
```

Now that the data is accessible from DuckDB, we can create a table in our database using a standard SQL query. 

```{r}
DBI::dbSendQuery(con, "CREATE TABLE dataset AS SELECT * FROM RP2017")
```

Before directly manipulating the data, let's examine the storage performance of different formats.

```{r}
disk_size <- fs::dir_info(here::here("tp", "data"), recurse = TRUE) |>
  filter(type == "file") |>
  mutate(name = basename(path)) |>
  arrange(name) |>
  group_by(name) |>
  summarise(total = sum(size)) |>
  ungroup() |>
  mutate(
    name = case_when(
      startsWith(name, "part-0") ~ "partitioned parquet",
      TRUE ~ tools::file_ext(name)
    ),
    name = forcats::fct_reorder(name, total)
  ) |>
  filter(name != "wal")

disk_size |>
  ggplot(aes(name, total)) +
  geom_col(fill = "#003299", width = 0.6) +
  geom_text(aes(label = glue::glue("{total}B")), hjust = 1, nudge_x = 0.4, size = 3.5, fontface = "bold") +
  coord_flip() +
  scale_y_continuous(labels = scales::label_bytes(units = "auto_si", accuracy = 1)) +
  labs(
    title = "Disk Space Size",
    subtitle = "between CSV, Parquet, Partitioned Parquet, and DuckDB",
    x = "Format",
    y = "Size"
  ) +
  theme_light()
```

On libère de la mémoire.

```{r}
rm(df_arrow)
```

### B - Query Performance Comparison

To compare different methods, we will execute a relatively simple query, aiming to determine the percentage of makeshift accommodations in each commune in France. In our dataset, makeshift accommodations are coded on the variable `TYPL` with modality 5. We will also use a correspondence table of communes to associate the name of the commune with an Insee commune code, in order to add a join to our query.

#### 1) In-memory

The traditional method for handling non-massive data involves importing the entire dataset into memory and then performing various operations. Let's start by importing both datasets into memory.

```{r}
# Data in parquet format
data <- arrow::read_parquet(
  file = "data/data_recensement_2017.parquet"
)
```

```{r}
bucket <- arrow::s3_bucket(
  "projet-formation",
  endpoint_override = "minio.lab.sspcloud.fr"
)

# Correspondence table parquet format
table_communes <- arrow::read_parquet(
  file = bucket$path("diffusion/bceao/table_communes.parquet")
)
```

We then define our query that performs the following operations:

-   Calculate the number of households in each commune.
-   Calculate the number of makeshift accommodations in each commune.
-   Calculate the percentage of makeshift accommodations.
-   Perform a join with the commune correspondence table.
-   Select certain columns.

```{r}
t <- Sys.time()
data |>
  select(REGION, COMMUNE, TYPL) |>
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(TYPL %in% "5"),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |>
  arrange(-part_logements_fortune) |>
  left_join(table_communes, by = c("COMMUNE" = "Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune)

elapsed_in_memory <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

Next, we release the memory used by the objects.

```{r}
rm(data)
```

#### 2) Lazy Evaluation with Arrow

Arrow allows for lazy evaluation, performing calculations only when necessary, thus optimizing performance. Arrow imports into memory only the data strictly required for executing the query. To import data lazily, we can use the `open_dataset()` function.

```{r}
data_lazy <- arrow::open_dataset(
  source = "data/data_partition",
  partitioning = arrow::schema(REGION = arrow::utf8())
)
```

At this stage, the `data_lazy` object contains information about the dataset but not the actual data.

We can obtain the schema of the dataset,

```{r}
data_lazy$schema
```

the dimension of the dataset,
```{r}
dim(data_lazy)
```

and the name of the columns.

```{r}
names(data_lazy)
```

Thanks to the integration of Arrow with dplyr, we can use the same query as before.

```{r}
query <- data_lazy |>
  select(REGION, COMMUNE, TYPL) |>
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(TYPL %in% "5"),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |>
  left_join(table_communes, by = c("COMMUNE" = "Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune)
```

As you can see, when defining the query, the calculations have not been performed yet. However, Arrow has already identified the columns needed for the calculations.

```{r}
query
```

To trigger the calculations, there are two different methods, `collect()` and `compute()`. The first returns the result of the query in `tibble` format, while the second returns the result as an Arrow object.

```{r}
query |>
  collect()
```

For comparison purposes, we will execute the entire query:

```{r}
t <- Sys.time()

arrow::open_dataset(
  source = "data/data_partition",
  partitioning = arrow::schema(REGION = arrow::utf8())
) |>
  select(REGION, COMMUNE, TYPL) |>
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(TYPL %in% "5"),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |>
  left_join(table_communes, by = c("COMMUNE" = "Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune) |>
  collect()

elapsed_lazy <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

#### 3) DuckDB

Another alternative to Arrow that avoids importing data into memory is DuckDB. With DuckDB, we can define an SQL query to perform the calculations. To do this, we can start by reconnecting to the previously created database.

```{r}
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "data/data_recensement_2017.duckdb", read_only = FALSE)
```

```{r}
DBI::dbListTables(con)
```

We need to read our commune correspondence table, which is located on the internet at the following address: [https://minio.lab.sspcloud.fr/projet-formation/diffusion/bceao/table_communes.parquet](https://minio.lab.sspcloud.fr/projet-formation/diffusion/bceao/table_communes.parquet). To avoid downloading the database locally, we read it directly. However, for this, we need to install the `httpfs` extension in DuckDB.

```{r}
DBI::dbSendQuery(con, "INSTALL httpfs; LOAD httpfs;")
```

```{r}
query_sql <- "
WITH t1 as(
  SELECT COMMUNE,
       COUNT(*) FILTER (WHERE TYPL = '5') AS nb_logements_fortune,
       COUNT(*) AS nb_logements_commune,
       nb_logements_fortune / nb_logements_commune AS part_logements_fortune
       FROM read_parquet('data/data_partition/*/*.parquet', hive_partitioning = 1)
       GROUP BY ALL
)
SELECT
      tc.Département, t1.*
      FROM t1
      LEFT JOIN (select Département, \"Code INSEE\" as codgeo
                 from 'https://minio.lab.sspcloud.fr/projet-formation/diffusion/bceao/table_communes.parquet') tc
      ON t1.COMMUNE = tc.codgeo
      ORDER BY part_logements_fortune DESC ;"
```

Once the query is defined, all that's left is to execute it and retrieve the result using the `dbGetQuery()` function.

```{r}
DBI::dbGetQuery(con, query_sql) |> as_tibble()
```

Similarly to before, let's execute the query in its entirety:

```{r}
t <- Sys.time()

query_sql <- "
WITH t1 as(
  SELECT COMMUNE,
       COUNT(*) FILTER (WHERE TYPL = '5') AS nb_logements_fortune,
       COUNT(*) AS nb_logements_commune,
       nb_logements_fortune / nb_logements_commune AS part_logements_fortune
       FROM read_parquet('data/data_partition/*/*.parquet', hive_partitioning = 1)
       GROUP BY ALL
)
SELECT
      tc.Département, t1.*
      FROM t1
      LEFT JOIN (select Département, \"Code INSEE\" as codgeo
                 from 'https://minio.lab.sspcloud.fr/projet-formation/diffusion/bceao/table_communes.parquet') tc
      ON t1.COMMUNE = tc.codgeo
      ORDER BY part_logements_fortune DESC ;"

DBI::dbGetQuery(con, query_sql) |> as_tibble()

elapsed_duckdb_sql <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

When data manipulations are finished, don't forget to close the connection.

```{r}
duckdb::dbDisconnect(con, shutdown = TRUE)
```

We free up the memory

```{r}
rm(con)
```

If you prefer not to use SQL, you can still leverage the speed of DuckDB while retaining the same dplyr query. To do this, use the `to_duckdb` function.

```{r}
query <- data_lazy |>
  arrow::to_duckdb() |>
  select(REGION, COMMUNE, TYPL) |>
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(as.integer(TYPL %in% "5")),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |>
  left_join(table_communes |> arrow::to_duckdb(),
    by = c("COMMUNE" = "Code INSEE")
  ) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune)
```

```{r}
query |>
  collect()
```

We calculate the execution time of this method:

```{r}
t <- Sys.time()

arrow::open_dataset(
  source = "data/data_partition",
  partitioning = arrow::schema(REGION = arrow::utf8())
) |>
  arrow::to_duckdb() |>
  select(REGION, COMMUNE, TYPL) |>
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(as.integer(TYPL %in% "5")),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |>
  left_join(table_communes |> arrow::to_duckdb(),
    by = c("COMMUNE" = "Code INSEE")
  ) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune)

elapsed_duckdb_dplyr <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

And if you want to know the underlying SQL query being used, you can always display it using the `show_query()` function.

```{r}
query |>
  show_query()
```

#### 4) Results

```{r}
performance <- tibble(
  format_text = c("in memory", "arrow lazy", "duckDB SQL", "duckDB dplyr"),
  elapse_time = round(c(elapsed_in_memory, elapsed_lazy, elapsed_duckdb_sql, elapsed_duckdb_dplyr), 3)
)

performance |>
  mutate(format_text = forcats::fct_reorder(format_text, elapse_time)) |>
  ggplot(aes(format_text, elapse_time)) +
  geom_col(fill = "#003299", width = 0.6) +
  geom_text(aes(label = glue::glue("{elapse_time} sec")), hjust = 1, nudge_y = 0.1, nudge_x = 0.4, size = 4, fontface = "bold") +
  coord_flip() +
  labs(
    title = "Query Performances",
    x = "",
    y = "Computation Time in Seconds"
  ) +
  theme_light()
```


## II - Cloud Data Manipulation

The Cloud offers many advantages, including the ability to manipulate data without storing it locally, as well as increased computing power. On the SSP Cloud, data is stored on MinIO, an open-source implementation of S3 object storage.

### A - Arrow

Arrow allows direct access to data stored in the cloud and performing queries on it. Let's start by accessing the bucket that stores the data needed for this session using the `get_bucket()` function.

```{r}
aws.s3::get_bucket("projet-formation", prefix = "diffusion/bceao", region = "")
```

We can see that several files are present in the bucket, including the `data_partition` folder that contains the partitioned Parquet files. To access the folder directly in the cloud without having to import them locally, we can use the same `open_dataset()` function as before. However, it is necessary to provide it with the path to our bucket, similar to specifying the path of a local file.

```{r}
bucket <- arrow::s3_bucket(
  "projet-formation",
  endpoint_override = "minio.lab.sspcloud.fr"
)

path <- bucket$path("diffusion/bceao/data_partition")

data_cloud <- arrow::open_dataset(
  source = path,
  partitioning = arrow::schema(REGION = arrow::utf8())
)
```

Similarly, we can define a query and then compute it. It's important to note that executing the query may take longer since the files are not stored locally, and there may be intra-cluster network streams. You can also observe the evolution of RAM usage during the query execution.

```{r}
query <- data_cloud |>
  select(REGION, COMMUNE, TYPL) |>
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(TYPL %in% "5"),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |>
  left_join(table_communes, by = c("COMMUNE" = "Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune)
```

```{r}
query |> collect()
```

### B - Spark

When data volumes exceed the capacities of standard machines (e.g., > 100GB), it becomes necessary to move to distributed infrastructures and adapt processing methods to distribute calculations. The distributed computing framework is `Spark`, which has widely popularized distributed computing by creating simple abstraction layers on top of the Map-Reduce computation, the basis of distributed computing.

In our case, we will perform `Spark` on `Kubernetes`. The following diagram illustrates this process. The `RStudio` service in which we are located will play the roles of both client and driver. When a distributed computation is requested, the driver will ask the `Kubernetes` API server to launch a pre-configured number of executors (in our case, 10), each of which will perform the requested tasks on a portion of the data. Once the desired aggregations have been calculated, we can retrieve them on the driver (the `RStudio`) for further processing in `R`, as their volume is now manageable.

![Spark on Kubernetes](img/spark-on-kube.png)

The `RStudio` instance we are using has `Spark` installed and is pre-configured to be accessible from `R`. To establish the connection, we will use the `sparklyr` package.

```{r}
library(sparklyr)
```

Let's start by configuring `Spark`, following the [documentation](https://spark.rstudio.com/guides/connections.html). Here is the meaning of the different configurations:

- `conf$sparklyr.defaultPackages`: Configuring `Spark` to directly retrieve data from S3 storage (`MinIO` on the SSP Cloud);
- `conf$spark.executor.instances`: The number of executors on which calculations will be distributed;
- `conf$spark.driver.memory`: The amount of RAM for the driver;
- `conf$spark.executor.memory`: The amount of RAM for each executor.

```{r}
conf <- spark_config()

conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:3.3.6"
conf$spark.executor.instances <- "10"
conf$spark.driver.memory <- "2GB"
conf$spark.executor.memory <- "2GB"

sc <- spark_connect(
  master = "k8s://https://kubernetes.default.svc:443",
  config = conf
)
```

We can check that `Kubernetes` has indeed launched the requested 10 executors. More concretely, these are 10 "clones" that are ready to perform the calculations assigned to them.

```{bash , echo=TRUE}
kubectl get pods -l spark-role=executor
```

The first step is to load the data onto the various executors. We will use the partitioned data available on the training MinIO bucket. The data is never local; if we were working with very large data, we couldn't load it into local storage anyway. It is loaded directly from our MinIO storage.

```{r}
data_spark <- spark_read_parquet(sc,
  path = "s3a://projet-formation/diffusion/bceao/data_partition",
  infer_schema = TRUE,
  memory = FALSE
)

table_communes_spark <- spark_read_parquet(sc,
  path = "s3a://projet-formation/diffusion/bceao/table_communes.parquet",
  infer_schema = TRUE,
  memory = FALSE
)
```

Several things are interesting to note at this stage:

- The parameter `infer_schema = TRUE` means that the schema (name and type of variables) will be automatically inferred from the data. This is a great strength of `Parquet`: the schema is directly stored in the file's metadata;
- The parameter `memory = FALSE` means that the data is not cached on the executors. Therefore, data loading is immediate as it is lazy: the data will be loaded on-the-fly when a query is actually launched. If we planned to make many successive queries with this data (and our infrastructure allows it), we could prefer to cache them. In this case, importing the data would take much longer, but subsequent queries would be very fast. A trade-off to make according to your use case!

Let's now define the query that our executors will have to perform. We take the same query as before - adjusting it very slightly. This highlights the great strength of the `sparklyr` package: we do `Spark` without even realizing it, as the syntax is that of `dplyr`.

```{r}
query <- data_spark |>
  select(REGION, COMMUNE, TYPL) |>
  mutate(is_logement_fortune = ifelse(TYPL == "5", 1, 0)) |>
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(is_logement_fortune),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |>
  left_join(table_communes_spark, by = c("COMMUNE" = "Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune)
```


Already? Be careful, once again, the queries performed by `Spark` are `lazy`: `Spark` will read the query and convert it into a logical and physical plan to optimize the processing (selecting relevant columns and rows from the `Parquet` file, optimizing processing steps, etc.). As before, the query is only actually launched on the cluster at the time of the `collect` operation.

```{r}
data_final <- collect(query)
```

Rather quick, but slower than with `Arrow`. That's logical! With `Arrow`, we perform in-memory computing on a single machine. With `Spark`, we also perform in-memory computing, but in a distributed manner across multiple machines, which implies parallelization costs upstream and downstream of the query. `Spark` doesn't show its full potential here because we are dealing with large-sized data, but not truly massive. In the case of truly massive data (several tens or hundreds of GB of data), only `Spark` would have been able to process such data.

Finally, it is possible to save the result of our various calculations directly in our own cloud storage space.

```{r}
MYBUCKET <- "MY_USERNAME"

bucket <- arrow::s3_bucket(
  MYBUCKET,
  endpoint_override = "minio.lab.sspcloud.fr"
)

path_out <- bucket$path("data_post_query.parquet")
arrow::write_dataset(data_final, path_out)
```
