---
title: "Manipulation de données massives"
editor: visual
---

# Manipulation de données massives avec R

L'objectif de cette session est de vous familiariser avec les principes et les techniques essentiels pour travailler avec des données massives. Les outils que nous allons explorer ont chacun leurs avantages et inconvénients, et le choix de les utiliser dépendra principalement des besoins spécifiques de vos projets. Il est important de noter que certains de ces outils nécessitent des infrastructures informatiques robustes qui peuvent ne pas être disponibles pour tous les projets.

Pour illustrer ces techniques, nous utiliserons les données du recensement de la population française de 2017. Bien que ces données ne soient pas considérées comme massives en soi, elles sont déjà assez volumineuses pour poser des défis de gestion de la mémoire sur certains ordinateurs. Notre objectif est de vous fournir les compétences nécessaires pour manipuler de grandes quantités de données, que vous pourrez ensuite appliquer à vos propres cas d'usage, qu'ils soient de grande ou de petite envergure.

Au cours de ce TP, nous explorerons deux approches principales. Tout d'abord, nous examinerons comment manipuler efficacement des ensembles de données massifs en utilisant des ressources **locales**, même lorsque la capacité de la mémoire RAM est inférieure à la taille des données analysées. Ensuite, nous plongerons dans la manipulation de données directement depuis le **cloud**, lorsque les ressources locales ne sont plus suffisantes pour répondre à vos besoins.

## I - Manipulation de données en local

Avant de commencer, chargeons quelques librairies qui nous seront utiles ultérieurement et définissons quelques réglages.

```{r}
library(ggplot2)
library(dplyr)

# Réglages arrow
arrow:::set_cpu_count(30)
options(arrow.use_threads = TRUE)
```

### A - Importation et stockage de données massives

#### Importation

Lorsque l'on parle de *Big Data*, le choix du format de stockage est essentiel pour optimiser l'efficacité et la performance des opérations. Le format le plus couramment utilisé pour le stockage de données est le `.csv`. Ce format présente l'avantage d'être lisible facilement, à la fois par les humains et par les différents langages et systèmes informatiques. Cependant, il a quelques inconvénients majeurs : il n'est pas compressé, ce qui signifie qu'il occupe beaucoup d'espace disque, et il doit être chargé intégralement en mémoire, même lorsque nous souhaitons analyser uniquement un sous-échantillon des données. Cette exigence a des implications à la fois en termes de stockage et de temps de calcul.

Dans ce TP, nous explorerons une alternative de plus en plus répandue au format `.csv` : le format `.parquet`. Ce format a été choisi comme format de stockage et de diffusion par défaut pour les prochaines années par l'Insee.

Pour commencer, nous importerons le fichier `tp/data/data_recensement_2017.csv` mis à votre disposition, qui contient les données du recensement de 2017. Nous utiliserons trois méthodes différentes pour l'importation, et nous analyserons leurs performances respectives :

-   importation via le package standard `readr`.

-   importation via le package `data.table`, qui est une extension au data.frame de R base.

-   importation via le package `arrow`.

L'objectif est de comprendre comment chaque méthode gère efficacement les données (massives), en tenant compte de l'occupation de la mémoire et du temps nécessaire pour effectuer l'importation. Cette étape est cruciale pour travailler avec des données massives de manière efficiente. Commencez par l'importation des données à l'aide de ces trois approches différentes.

```{r}
# Importation avec readr
t <- Sys.time()
df_readr <- readr::read_delim(
  "data/data_recensement_2017.csv",
  delim = ";",
  col_types = readr::cols(COMMUNE = readr::col_character())
)
elapsed_readr <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

```{r}
# Importation avec data.table
t <- Sys.time()
df_datatable <- data.table::fread(
  "data/data_recensement_2017.csv",
  sep = ";",
  colClasses = c(
    COMMUNE = "character"
  ),
  data.table = FALSE
)
elapsed_datatable <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

```{r}
# Importer avec arrow
t <- Sys.time()
df_arrow <- arrow::read_delim_arrow(
  "data/data_recensement_2017.csv",
  delim = ";",
  col_types = arrow::schema(COMMUNE = arrow::utf8()),
  as_data_frame = TRUE
)
elapsed_arrow <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

```{r}
head(df_arrow, 6)
```

Il est judicieux de supprimer les objets lourds dont on n'a plus besoin dans l'environnement R. En effet, lorsque vous créez des objets dans R, ils occupent de l'espace en mémoire. Si vous accumulez de nombreux objets sans les supprimer, cela peut rapidement saturer la mémoire disponible, ce qui peut entraîner des ralentissements ou des crash de votre session R. En supprimant les objets inutiles à l'aide de la fonction `rm()`, vous libérez de l'espace mémoire pour d'autres tâches ou analyses. De plus, moins la mémoire est encombrée plus les performances seront rapides. Supprimons donc les deux objets issus de l'importation via `readr` et `data.table`.

```{r}
# On libère de la mémoire
rm(df_readr, df_datatable)
gc()
```

On utilise également la fonction `gc()`, qui sert à déclencher la collecte des ordures (garbage collection) dans R. La collecte des ordures est un processus par lequel R identifie et libère la mémoire occupée par des objets qui ne sont plus référencés ni utilisés. Bien que R effectue généralement la collecte des ordures automatiquement, l'appel manuel à `gc()` peut être utile dans les situations où vous souhaitez vous assurer que la mémoire est correctement nettoyée, notamment après avoir supprimé de gros objets à l'aide de `rm()`.

Analysons les différences de temps d'import du CSV entre les 3 méthodes.

```{r}
performance <- tibble(format_text = c("readr", "data.table", "arrow"),
                             elapse_time = round(c(elapsed_readr, elapsed_datatable, elapsed_arrow),3))


performance |> 
  mutate(format_text = forcats::fct_reorder(format_text, elapse_time)) |> 
  ggplot(aes(format_text, elapse_time))+
  geom_col(fill = "#003299", width=0.6)+
  geom_text(aes(label = glue::glue("{elapse_time} sec")), hjust = -0.1, size = 4, fontface = "bold") +
  expand_limits(y = c(0, 200)) +
  coord_flip()+
  labs(title = "Performances de l'importation d'un fichier CSV",
       subtitle ="entre readr, data.table et arrow",
       x = "",
       y = "Temps d'import en secondes")+
  theme_light()
```

A la vue des résultats, il est évident que les bibliothèques `data.table` et `arrow` se révèlent être des choix particulièrement adaptés pour l'importation de fichiers CSV de grande taille dans R. En effet, l'utilisation de `data.table` permet de diviser pratiquement par deux le temps nécessaire à l'importation, tandis que l'impact positif de la bibliothèque `arrow` sur les performances est encore plus spectaculaire.

Comme nous l'avons mentionné précédemment, le format `.csv` n'est pas l'option idéale pour la manipulation de données massives. Par conséquent, il est fortement recommandé de travailler avec des données au format .parquet ou de les stocker dans une base de données, en fonction des besoins spécifiques de votre projet. Il convient de noter que, selon les cas d'utilisation, il peut être judicieux de travailler avec des fichiers parquet partitionnés plutôt qu'avec un seul gros fichier .parquet. Cette approche permet une gestion plus efficace des données massives tout en améliorant les performances de traitement.

#### Stockage

À présent que nous avons importé notre fichier CSV en mémoire, passons à l'étape suivante : écrire le fichier `data/data_recensement_2017.parquet`.

```{r}
# Ecriture d'un fichier parquet unique
arrow::write_parquet(df_arrow, "data/data_recensement_2017.parquet")
```

On peut également choisir de partitionner notre fichier parquet, par exemple par région. On exporte le fichier partionné dans un dossier `data/data_partition`.

```{r}
# Ecriture d'un dossier contenant des fichiers parquet partitionnés par la région
arrow::write_dataset(df_arrow,
                     "data/data_partition", 
                     format = "parquet",
                     partitioning = c("REGION"))
```

Prenez le temps d'examiner la manière dont les fichiers parquet sont sauvegardés dans le dossier `data_partition`.

```{r}
fs::dir_tree("data/data_partition")
```

Une autre méthode efficace pour stocker nos données consiste à les intégrer dans une base de données DuckDB, ce qui nous permettra ensuite d'effectuer des requêtes SQL. Bien que DuckDB ne puisse pas encore ouvrir des fichiers multi-niveau créés par partition, il offre la possibilité de lire directement des fichiers CSV ou Parquet. Cependant, il est important de noter que, comme toute base de données, le format `.duckdb` peut contenir plusieurs tables et exécuter toutes les opérations standards de gestion de base de données.

Pour créer une database depuis R, il suffit d'utiliser la fonction `duckdb()` en spécifiant le chemin où vous souhaitez l'enregistrer.

```{r}
con <- DBI::dbConnect(duckdb::duckdb(), dbdir="data/data_recensement_2017.duckdb", read_only=FALSE)
```

L'un des principaux avantages de DuckDB est son interopérabilité avec Arrow, ce qui permet de très facilement passer d'un object arrow à un object duckdb et vice-versa. Pour cela il suffit d'utiliser les fonctions `to_duckdb()` et `to_arrow()`.

```{r}
arrow::to_duckdb(df_arrow, table_name = "RP2017", con = con)
```

Maintenant que les données sont accessibles depuis DuckDB, nous pouvons créer une table dans notre base de données en utilisant une requête SQL standard.

```{r}
DBI::dbSendQuery(con, "CREATE TABLE dataset AS SELECT * FROM RP2017")
```

Avant de manipuler directement les données, regardons les performances de stockage des différents formats.

```{r}
disk_size <- fs::dir_info(here::here("tp", "data"), recurse = TRUE) |>
  filter(type == "file") |>
  mutate(name = basename(path)) |> 
  arrange(name) |>
  group_by(name) |> 
  summarise(total = sum(size)) |> 
  ungroup() |>
  mutate(name = case_when(
    startsWith(name, "part-0") ~ "parquet partitionné",
    TRUE ~ tools::file_ext(name)
  ),
  name = forcats::fct_reorder(name, total)
  ) |> 
  filter(name != "wal")

disk_size |> 
  ggplot(aes(name, total)) +
  geom_col(fill = "#003299", width=0.6)+
  geom_text(aes(label = glue::glue("{total}B")), hjust = 1, nudge_x = 0.4, size = 3.5, fontface = "bold") +
  coord_flip()+
  scale_y_continuous(labels = scales::label_bytes(units = "auto_si", accuracy = 1))+
  labs(title = "Taille sur l'espace disque",
       subtitle ="entre CSV, Parquet, Parquet partitionné et DuckDB",
       x = "Format",
       y = "Size")+
  theme_light()
```

On libère de la mémoire.

```{r}
rm(df_arrow)
gc()
```

### B - Comparaison des performances de requêtes

Pour comparer différentes méthodes, nous allons exécuter une requête relativement simple, dont l'objectif est de déterminer le pourcentage de logements dits de "fortune" dans chaque commune de France. Dans notre jeu de données, les logements de fortune sont codés sur la variable `TYPL` avec la modalité 5. Nous allons également utiliser une table de correspondance des communes pour associer le nom de la commune à un code commune Insee, de sorte à rajouter une jointure à notre requête.

#### 1) In-memory

La méthode traditionnelle pour manipuler des données non massives consiste à importer l'ensemble des données en mémoire et à effectuer ensuite différentes opérations. Commençons donc par importer les deux jeux de données en mémoire.

```{r}
# jeu de données au format parquet
data <- arrow::read_parquet(
  file = "data/data_recensement_2017.parquet"
)
```

```{r}
# table de correspondance au format parquet
table_communes <- arrow::read_parquet(
  file = arrow::s3_bucket(
    "projet-formation/diffusion/bceao/table_communes.parquet",
    endpoint_override = "minio.lab.sspcloud.fr"
  )
)
```

Nous définissons ensuite notre requête qui réalise les opérations suivantes :

-   Calcule le nombre de logements dans chaque communes.
-   Calcule le nombre de logements de fortune dans chaque communes.
-   Calcule la part de logement de fortune.
-   Réalise une jointure avec la table de correspondance des communes.
-   Sélectionne certaines de colonne

```{r}
t <- Sys.time()
data |> 
  select(REGION, COMMUNE, TYPL) |> 
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(TYPL %in% "5"),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |>
  arrange(-part_logements_fortune) |>
  left_join(table_communes, by=c("COMMUNE"="Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune)

elapsed_in_memory <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

Ensuite, nous libérons la mémoire utilisée par les objets.

```{r}
rm(data)
gc()
```

#### 2) Lazy evaluation avec Arrow

Arrow permet d'effectuer des évaluations en mode *"lazy"*. Cette méthode réalise les calculs uniquement lorsque c'est nécessaire, optimisant ainsi les performances. Arrow n'importe en mémoire que les données strictement nécessaires à l'exécution de la requête. Pour importer les données de manière *lazy* on peut utiliser la fonction `open_dataset()`

```{r}
data_lazy <- arrow::open_dataset(
  source = "data/data_partition",
  partitioning = arrow::schema(REGION = arrow::utf8()))
```

À ce stade, l'objet `data_lazy` contient des informations sur le jeu de données, mais pas les données elles-mêmes.

Nous pouvons obtenir le schéma du jeu de données,

```{r}
data_lazy$schema
```

la dimension du jeu de données,

```{r}
dim(data_lazy)
```

ainsi que les noms des colonnes.

```{r}
names(data_lazy)
```

Grâce à l'intégration d'Arrow avec dplyr on peut reprendre la même requête que précedemment.

```{r}
query <- data_lazy |> 
  select(REGION, COMMUNE, TYPL) |> 
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(TYPL %in% "5"),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |> 
  left_join(table_communes, by=c("COMMUNE"="Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune) 
```

Comme vous pouvez le constater, lors de la définition de la requête, les calculs n'ont pas encore été réalisés. Cependant, arrow a d'ores et déjà identifié les colonnes nécessaires aux calculs.

```{r}
query 
```

Pour enclencher les calculs il existe deux méthodes différentes, `collect()` et `compute()`. La première renvoie le résultat de la requête au format `tibble` tandis que la seconde renvoie le résultat sous la forme d'un objet Arrow.

```{r}
t <- Sys.time()
query |> 
  collect()
elapsed_lazy <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

#### 3) DuckDB

Une autre alternative à Arrow qui évite d'importer les données en mémoire est DuckDB. En effet, grâce à DuckDB on peut définir une requête SQL pour réaliser les calculs. Pour cela, on peut commencer par se reconnecter à la base de données précédemment créée.

```{r}
con <- DBI::dbConnect(duckdb::duckdb(), dbdir="data/data_recensement_2017.duckdb", read_only=FALSE)
```

```{r}
DBI::dbListTables(con)
```

Pour le moment, seule la table "dataset" est présente dans notre base de données. Nous devons ajouter notre table de correspondance des communes. Pour éviter de télécharger la base de données localement, nous la lisons directement depuis le cloud où elle est sauvegardée. Pour cela, nous avons besoin d'installer l'extension `httpfs` dans DuckDB.

```{r}
DBI::dbSendQuery(con, "INSTALL httpfs;")
```

Nous pouvons maintenant ajouter la table que nous appelons `corres`.

```{r}
DBI::dbSendQuery(con, "CREATE TABLE corres AS SELECT * FROM read_parquet('https://minio.lab.sspcloud.fr/projet-formation/diffusion/bceao/table_communes.parquet');")
DBI::dbListTables(con)
```

La table `corres` a bien été rajoutée à notre base de données ! Il ne manque plus qu'à traduire notre requête précedente au format SQL.

```{r}
query_sql <- "SELECT
                tc.Commune,
                tc.Département,
                nb_logements_fortune,
                nb_logements_commune,
                part_logements_fortune
              FROM (
                SELECT
                  dataset.COMMUNE,
                  SUM(CASE WHEN dataset.TYPL = '5' THEN 1 ELSE 0 END) AS nb_logements_fortune,
                  COUNT(*) AS nb_logements_commune,
                  SUM(CASE WHEN dataset.TYPL = '5' THEN 1 ELSE 0 END) / COUNT(*) AS part_logements_fortune
                FROM
                  dataset
                GROUP BY
                dataset.COMMUNE
              ) AS subquery
              LEFT JOIN
                corres AS tc
              ON
                subquery.COMMUNE = tc.\"Code INSEE\"
              ORDER BY
                part_logements_fortune DESC;"
```

Une fois la requête définie il ne reste plus qu'à l'executer et récupérer le résultatà l'aide de la fonction `dbGetQuery()`.

```{r}
t <- Sys.time()
DBI::dbGetQuery(con, query_sql) |> as_tibble()
elapsed_duckdb_sql <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

Lorsque les manipulations de données sont terminées, n'oubliez pas de fermer la connexion.

```{r}
duckdb::dbDisconnect(con, shutdown=TRUE)
```

On libère de la mémoire.

```{r}
rm(con)
gc()
```

Si vous préférez ne pas utiliser SQL, vous pouvez toujours profiter de la rapidité de DuckDB tout en conservant la même requête dplyr. Pour cela, utilisez la fonction `to_duckdb`.

```{r}
query <- data_lazy |>
  arrow::to_duckdb() |>
  select(REGION, COMMUNE, TYPL) |> 
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(as.integer(TYPL %in% "5")),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |> 
  left_join(table_communes |> arrow::to_duckdb()
            , by=c("COMMUNE"="Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune) 
```

```{r}
t <- Sys.time()
query |> 
  collect()
elapsed_duckdb_dplyr <- as.numeric(difftime(Sys.time(), t, units = "secs"))
```

Et si vous souhaitez connaître la requête SQL qui est utilisée sous le capot, vous pouvez toujours l'afficher grâce à la fonction `show_query()`.

```{r}
query |>
  show_query()
```

#### 4) Résultats

```{r}
performance <- tibble(format_text = c("in memory", "arrow lazy", "duckDB SQL", "duckDB dplyr"),
                             elapse_time = round(c(elapsed_in_memory, elapsed_lazy, elapsed_duckdb_sql, elapsed_duckdb_dplyr),3))

performance |> 
  mutate(format_text = forcats::fct_reorder(format_text, elapse_time)) |> 
  ggplot(aes(format_text, elapse_time))+
  geom_col(fill = "#003299", width=0.6)+
  geom_text(aes(label = glue::glue("{elapse_time} sec")), hjust = 1, nudge_y = 0.1, nudge_x = 0.4, size = 4, fontface = "bold") +
  coord_flip()+
  labs(title = "Performances des requêtes",
       x = "",
       y = "Temps de calcul en secondes")+
  theme_light()
```

## II - Manipulation de données sur le Cloud

Le Cloud offre de nombreux avantages, notamment la possibilité de manipuler des données sans avoir à les stocker localement, ainsi qu'une puissance de calcul accrue. Sur le SSP Cloud, les données sont stockées sur MinIO, une implémentation open-source du stockage objet de type S3.

### A - Arrow

Arrow permet d'accéder directement à des données stockées sur le cloud et d'y effectuer des requêtes. Commençons par accéder au bucket qui stocke les données nécessaires à cette formation en utilisant la fonction `get_bucket()`.

```{r}
aws.s3::get_bucket("projet-formation", prefix = "diffusion/bceao", region = "")
```

Nous pouvons constater que plusieurs fichiers sont présents dans le bucket, dont le dossier `data_partition` qui contient les fichiers Parquet partitionnés. Pour accéder au dossier directement sur le Cloud sans avoir à les importer localement, nous pouvons utiliser la même fonction `open_dataset()` que précédemment. Cependant, il est nécessaire de lui fournir le chemin vers notre bucket, de la même manière que l'on spécifie le chemin d'un fichier local.

```{r}
bucket <- arrow::s3_bucket(
  "projet-formation/diffusion/bceao/data_partition",
  endpoint_override = "minio.lab.sspcloud.fr"
)

data_cloud <- arrow::open_dataset(
  source = bucket,
  partitioning = arrow::schema(REGION = arrow::utf8()))

```

De manière tout à fait similaire, nous pouvons définir une requête, puis la calculer. Il est important de noter que l'exécution de la requête peut prendre plus de temps, puisque les fichiers n'étant pas stockés localement il peut y avoir des flux réseaux intra-cluster. Vous pouvez également observer l'évolution de l'utilisation de la RAM lors de l'exécution de la requête.

```{r}
query <- data_cloud |> 
  select(REGION, COMMUNE, TYPL) |> 
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(TYPL %in% "5"),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |> 
  left_join(table_communes, by=c("COMMUNE"="Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune)
```

```{r}
query |> collect()
```

### B - Spark

Lorsque les volumes de données dépassent les capacités de machines standards (ex : > 100Go), il faut passer sur des infrastructures distribuées, et adapter les méthodes de traitement pour pouvoir distribuer les calculs. Le framework de calcul distribué est `Spark`, qui a largement démocratisé le calcul distribué en créant des couches d'abstraction simples sur le calcul Map-Reduce, base du calcul distribué.

Dans notre cas, nous allons effectuer du `Spark` sur `Kubernetes`. Le schéma suivant illustre ce processus. Le service `RStudio` dans lequel on se trouve va jouer à la fois le rôle de client et de driver. Lorsqu'un calcul distribué sera demandé, le driver va demander à l'API server de `Kubernetes` de lancer un nombre pré-configuré d'exécuteurs (dans notre cas, 10) qui vont chacun réaliser les tâches demandées sur une partie des données. Une fois que les agrégations voulues ont été calculées, on pourra les récupérer sur le driver (le `RStudio`) pour les traiter en `R`, leur volumétrie étant à présent gérable.

![](img/spark-on-kube.png){fig-align="center"}



Nous allons comme auparavant créer un SparkContext sur un notebook R. Ensuite ce programme "driver" se connectera à un cluster manager de type Kubernetes pour pouvoir lancer des workers. Le schéma traditionnel pour comprendre l'architecture est le suivant :


Onyxia injecte une configuration par défaut pour votre configuration Spark. Tous ces éléments sont ceux qu'Onyxia injecte à votre place. Vous pouvez les surcharger au runtime.

```{r}
library(sparklyr)
```

```{r}
conf <- spark_config()
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:3.3.6"

conf$spark.executor.instances <- "10"
conf$spark.driver.memory <- "2GB"
conf$spark.executor.memory <- "2GB"

sc <- spark_connect(master = "k8s://https://kubernetes.default.svc:443", 
                    config = conf)
```

```{bash , echo=TRUE}
kubectl get pods -l spark-role=executor
```

```{r}
data_spark <- spark_read_parquet(sc,
                                 path = "s3a://projet-formation/diffusion/bceao/data_partition",
                                 infer_schema = TRUE,
                                 memory = FALSE
)
```

```{r}
table_communes_spark <- spark_read_parquet(sc,
                                 path = "s3a://projet-formation/diffusion/bceao/table_communes.parquet",
                                 infer_schema = TRUE,
                                 memory = FALSE
)
```

```{r}
query <- data_spark |> 
  select(REGION, COMMUNE, TYPL) |> 
  mutate(is_logement_fortune = ifelse(TYPL == "5", 1, 0)) |>
  group_by(COMMUNE) |>
  summarise(
    nb_logements_fortune = sum(is_logement_fortune),
    nb_logements_commune = n()
  ) |>
  mutate(
    part_logements_fortune = nb_logements_fortune / nb_logements_commune
  ) |> 
  left_join(table_communes_spark, by=c("COMMUNE"="Code INSEE")) |>
  select(Commune, Département, nb_logements_fortune, nb_logements_commune, part_logements_fortune) |>
  arrange(-part_logements_fortune)
```

```{r}
data_final <- collect(query)
```
