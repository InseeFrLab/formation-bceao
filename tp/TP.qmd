---
title: "TP : traitement des données volumineuses"
format: html
editor: visual
---

# TP : traitement des données volumineuses

## Stockage Cloud

```{r}
library(aws.s3)
```

## Limite des fichiers CSV

```{r}
library(readr)
```

```{r}
aws.s3::get_bucket("projet-formation", prefix = "diffusion/bceao", region = "")
```

1GB = 10^9 B 
donnees_caisse.csv : 9 GB 
donnees_caisse_sample.csv : 16 KB

```{r}
df_sample <- s3read_using(
  FUN = read_csv,
  object = "diffusion/bceao/donnees_caisse_sample.csv",
  bucket = "projet-formation",
  opts = list("region" = "")
  )

head(df_sample)
```

```{r}
#  df_full <- s3read_using(
#    FUN = read_csv,
#    object = "diffusion/bceao/donnees_caisse.csv",
#    bucket = "projet-formation",
#    opts = list("region" = "")
#   )
```

Crash.. 8GB de mémoire pas suffisant pour une table CSV de 5GB En général, nécessaire d'avoir au moins 2 fois plus de RAM que la taille de la table à importer

## Le format Parquet

```{r}
library(arrow)
```

```{r}
bucket <- s3_bucket(bucket = "projet-formation",
                    access_key = Sys.getenv("AWS_ACCESS_KEY_ID"),
                    secret_key = Sys.getenv("AWS_SECRET_ACCESS_KEY"),
                    session_token = Sys.getenv("AWS_SESSION_TOKEN"),
                    endpoint_override = Sys.getenv("AWS_S3_ENDPOINT")
                    )
```

### Ecrire un fichier Parquet

```{r}
path_out <- bucket$path("diffusion/bceao/donnees_caisse.parquet")
write_parquet(df_full, parquet_good)
```

```{r}
aws.s3::get_bucket("projet-formation", region = "", prefix = "diffusion/bceao")
```

donnees_caisse.csv : 9 GB donnees_caisse.parquet : 3.3 GB

Taille presque divisée par 9 ! Bien plus facile à stocker, mais également bien plus performant à importer ensuite en mémoire.

### Partitionner un fichier Parquet

```{r}
path_out <- bucket$path("diffusion/bceao/part-test")
write_dataset(df_full, path_out, format = "parquet", partitioning  = "")
```

## Lire un fichier Parquet

### Données peu volumineuses : import en mémoire

### Données volumineuses : requêtes optimisées

## Le cas des données massives
